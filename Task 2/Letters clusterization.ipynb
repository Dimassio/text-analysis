{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Анализ текстов. Второе задание \"Кластеризация писем\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ds = pd.read_csv(\"Emails.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7945\n"
     ]
    }
   ],
   "source": [
    "messages = ds[\"ExtractedBodyText\"]\n",
    "print len(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Предобработка.\n",
    "Во-первых, есть много не нужных сообщений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# todo: mb попробовать отрезать длину?\n",
    "# texts = [text.decode('utf-8') for text in messages.astype(str).tolist() if (text.lower() != \"nan\") and (text.lower() != \"thx\") and (text.lower() != \"fyi\") ]\n",
    "texts = [text.decode('utf-8') for text in messages.astype(str).tolist() if (len(text) > 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'B6\\nThursday, March 3, 2011 9:45 PM\\nH: Latest How Syria is aiding Qaddafi and more... Sid\\nhrc memo syria aiding libya 030311.docx; hrc memo syria aiding libya 030311.docx\\nMarch 3, 2011\\nFor: Hillary', u'H <hrod17@clintonemail.com>\\nFriday, March 11, 2011 1:36 PM\\nHuma Abedin\\nFw: H: Latest: How Syria is aiding Qaddafi and more... Sid\\nhrc memo syria aiding libya 030311.docx\\nPis print.', u\"Pis print.\\n-\\u2022-...-^\\nH < hrod17@clintonernailcom>\\nWednesday, September 12, 2012 2:11 PM\\n\\xb0Russorv@state.gov'\\nFw: Meet The Right-Wing Extremist Behind Anti-fvluslim Film That Sparked Deadly Riots\\nFrom [meat)\\nSent: Wednesday, September 12, 2012 01:00 PM\\nTo: 11\\nSubject: Meet The Right Wing Extremist Behind Anti-Muslim Film That Sparked Deadly Riots\\nhtte/maxbiumenthal.com12012/09/meet-the-right-wing-extremist-behind-anti-musiim-tihn-that-sparked-\\ndeadly-riots/\\nSent from my Verizon Wireless 4G LTE DROID\\nU.S. Department of State\\nCase No. F-2015-04841\\nDoc No. C05739559\\nDate: 05/13/2015\\nSTATE DEPT. - PRODUCED TO HOUSE SELECT BENGHAZI COMM.\\nSUBJECT TO AGREEMENT ON SENSITIVE INFORMATION & REDACTIONS. NO FOIA WAIVER. STATE-5CB0045251\", u'H <hrod17@clintonemail.corn>\\nFriday, March 11, 2011 1:36 PM\\nHuma Abedin\\nFw: H: Latest: How Syria is aiding Qaddafi and more... Sid\\nhrc memo Syria aiding libya 030311.docx\\nPis print.', u'B6\\nWednesday, September 12, 2012 6:16 PM\\nFwd: more on libya\\nLibya 37 sept 12 12.docx\\nSending direct. Just in.\\nSent from my Verizon Wireless 4G LTE DRUID']\n",
      "5767\n"
     ]
    }
   ],
   "source": [
    "print texts[:5]\n",
    "print len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Во-вторых, попробуем убрать стоп слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Убираем стоп-слова\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words(\"english\")\n",
    "texts_without_stopwords = []\n",
    "for sample in texts:\n",
    "    texts_without_stopwords.append(' '.join([word for word in sample.split(' ') if word.lower() not in stop_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'B6\\nThursday, March 3, 2011 9:45 PM\\nH: Latest Syria aiding Qaddafi more... Sid\\nhrc memo syria aiding libya 030311.docx; hrc memo syria aiding libya 030311.docx\\nMarch 3, 2011\\nFor: Hillary', u'H <hrod17@clintonemail.com>\\nFriday, March 11, 2011 1:36 PM\\nHuma Abedin\\nFw: H: Latest: Syria aiding Qaddafi more... Sid\\nhrc memo syria aiding libya 030311.docx\\nPis print.', u\"Pis print.\\n-\\u2022-...-^\\nH < hrod17@clintonernailcom>\\nWednesday, September 12, 2012 2:11 PM\\n\\xb0Russorv@state.gov'\\nFw: Meet Right-Wing Extremist Behind Anti-fvluslim Film Sparked Deadly Riots\\nFrom [meat)\\nSent: Wednesday, September 12, 2012 01:00 PM\\nTo: 11\\nSubject: Meet Right Wing Extremist Behind Anti-Muslim Film Sparked Deadly Riots\\nhtte/maxbiumenthal.com12012/09/meet-the-right-wing-extremist-behind-anti-musiim-tihn-that-sparked-\\ndeadly-riots/\\nSent Verizon Wireless 4G LTE DROID\\nU.S. Department State\\nCase No. F-2015-04841\\nDoc No. C05739559\\nDate: 05/13/2015\\nSTATE DEPT. - PRODUCED HOUSE SELECT BENGHAZI COMM.\\nSUBJECT AGREEMENT SENSITIVE INFORMATION & REDACTIONS. FOIA WAIVER. STATE-5CB0045251\", u'H <hrod17@clintonemail.corn>\\nFriday, March 11, 2011 1:36 PM\\nHuma Abedin\\nFw: H: Latest: Syria aiding Qaddafi more... Sid\\nhrc memo Syria aiding libya 030311.docx\\nPis print.', u'B6\\nWednesday, September 12, 2012 6:16 PM\\nFwd: libya\\nLibya 37 sept 12 12.docx\\nSending direct. in.\\nSent Verizon Wireless 4G LTE DRUID']\n"
     ]
    }
   ],
   "source": [
    "print texts_without_stopwords[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top bigramms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk import FreqDist\n",
    "tokens=nltk.word_tokenize(''.join(texts_without_stopwords))\n",
    "bigrams=ngrams(tokens,2)\n",
    "freq_bigrams = nltk.FreqDist(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'.', u\"''\") 1387\n",
      "(u'.', u'``') 1245\n",
      "(u',', u\"''\") 1062\n",
      "(u'.', u'The') 716\n",
      "(u'Secretary', u\"'s\") 676\n",
      "(u',', u'2010') 596\n",
      "(u',', u'2009') 573\n",
      "(u'@', u'state.gov') 534\n",
      "(u\"'s\", u'Office') 483\n",
      "(u'.', u'I') 467\n",
      "(u'No', u'.') 461\n",
      "(u'United', u'States') 445\n",
      "(u'State', u'Department') 440\n",
      "(u'White', u'House') 411\n",
      "(u'state.gov', u'>') 399\n",
      "(u',', u'``') 398\n",
      "(u'I', u\"'m\") 374\n",
      "(u'do', u\"n't\") 359\n",
      "(u'Re', u':') 325\n",
      "(u')', u'.') 296\n",
      "(u'Department', u'State') 287\n",
      "(u'it', u\"'s\") 285\n",
      "(u')', u',') 282\n",
      "(u'it', u'.') 276\n",
      "(u'pm', u'Secretary') 273\n",
      "(u'Obama', u\"'s\") 266\n",
      "(u'.', u'-') 265\n",
      "(u'.', u'In') 265\n",
      "(u'U.S.', u'Department') 261\n",
      "(u',', u'the') 246\n"
     ]
    }
   ],
   "source": [
    "for bigram, value in freq_bigrams.most_common(30):\n",
    "    print bigram, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Коллокации из 2х слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramAssocMeasures\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "bigram_measures = BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(tokens)\n",
    "finder.apply_freq_filter(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'BINYAMIN', u'NETANYAHU'), (u'Chico', u'Menashe'), (u'Deep', u'Throat'), (u'HAMAD', u'BIN'), (u'Im', u'Tirtzu'), (u'Buenos', u'Aires'), (u'Belief', u'Blog'), (u'Elliott', u'Trudeau'), (u'Montreal-Pierre', u'Elliott'), (u'USNS', u'Comfort'), (u'ESMTP', u'id'), (u'Generating', u'server'), (u'Sensitive', u'Unclassified'), (u'Judea', u'Samaria'), (u'Iwo', u'Jima'), (u'Vital', u'Voices'), (u'Cape', u'Verde'), (u'MIDDLE', u'EAST'), (u'Maya', u'Angelou'), (u'Sao', u'Paulo')]\n"
     ]
    }
   ],
   "source": [
    "print finder.nbest(bigram_measures.pmi, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кластеризация с KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Признаки\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_df=500, min_df=10, ngram_range=(1, 3))\n",
    "data_set = [text for text in texts_without_stopwords if len(text) > 10]\n",
    "matrix = vectorizer.fit_transform(data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5573, 6091)\n"
     ]
    }
   ],
   "source": [
    "print matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Кластеризуем\n",
    "from sklearn.cluster import KMeans\n",
    "# 5 can interpret\n",
    "k = 6\n",
    "model = KMeans(n_clusters=k)\n",
    "preds = model.fit_predict(matrix.toarray())\n",
    "print(preds[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_of_samples = 10\n",
    "for j in range(k):\n",
    "    print \"====================================\"\n",
    "    print str(j) + \"CLASS:\"\n",
    "    d = model.transform(matrix.toarray())[:, j]\n",
    "    ind = np.argsort(d)[::][:num_of_samples]\n",
    "    for i in range(len(ind)):\n",
    "        print \"---------------------------\"\n",
    "        print data_set[ind[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
