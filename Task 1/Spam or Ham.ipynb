{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Анализ текстов. Первое задание \"Спам фильтр\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ds = pd.read_table(\"SMSSpamCollection.txt\", delimiter=\"\\t\", names=(\"spam_or_ham\", \"text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spam_or_ham</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  spam_or_ham                                               text\n",
       "0         ham  Go until jurong point, crazy.. Available only ...\n",
       "1         ham                      Ok lar... Joking wif u oni...\n",
       "2        spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3         ham  U dun say so early hor... U c already then say...\n",
       "4         ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = [text.decode(\"utf8\") for text in ds.text]\n",
    "labels = [0 if label == \"ham\" else 1 for label in ds.spam_or_ham]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 0, 1, 0, 0, 1, 1]\n",
      "[u'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...', u'Ok lar... Joking wif u oni...', u\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\"]\n"
     ]
    }
   ],
   "source": [
    "print labels[:10]\n",
    "print text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Получаем признаки\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.932640298361\n"
     ]
    }
   ],
   "source": [
    "# Классификация с лог регрессией\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "model = LogisticRegression()\n",
    "res = cross_val_score(model, X, labels, scoring=\"f1\", cv=10)\n",
    "print np.mean(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты:\n",
    "0.932640298361"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = [\n",
    "\"FreeMsg: Txt: CALL to No: 86888 & claim your reward of 3 hours talk time to use from your phone now! Subscribe6GB\",\n",
    "\"FreeMsg: Txt: claim your reward of 3 hours talk time\",\n",
    "\"Have you visited the last lecture on physics?\",\n",
    "\"Have you visited the last lecture on physics? Just buy this book and you will have all materials! Only 99$\",\n",
    "\"Only 99$\"\n",
    "]\n",
    "test_features = vectorizer.transform(test)\n",
    "\n",
    "test_labels = model.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты:\n",
    "1 1 0 0 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def n_gramm_testing_score(vectorizer, model, text, labels):\n",
    "    X_with_ngrams = vectorizer.fit_transform(text)\n",
    "    res = cross_val_score(model, X_with_ngrams, labels, scoring=\"f1\", cv=10)\n",
    "    return round(np.mean(res), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.82\n",
      "0.73\n",
      "0.93\n"
     ]
    }
   ],
   "source": [
    "# Добавляем в признаки n-граммы\n",
    "model = LogisticRegression()\n",
    "\n",
    "n_gram_vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "ngram_2_2_score = n_gramm_testing_score(n_gram_vectorizer, model, text, labels)\n",
    "print ngram_2_2_score\n",
    "\n",
    "n_gram_vectorizer = CountVectorizer(ngram_range=(3,3))\n",
    "ngram_3_3_score = n_gramm_testing_score(n_gram_vectorizer, model, text, labels)\n",
    "print ngram_3_3_score\n",
    "\n",
    "n_gram_vectorizer = CountVectorizer(ngram_range=(1,3))\n",
    "ngram_1_3_score = n_gramm_testing_score(n_gram_vectorizer, model, text, labels)\n",
    "print ngram_1_3_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат кросс валидации с использованием соответственно только 2грамм, 3грамм, 1-3грамм\n",
    "\n",
    "0.82, 0.73, 0.93"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.65\n",
      "0.38\n",
      "0.89\n"
     ]
    }
   ],
   "source": [
    "# Пробуем н-граммы с наивным Байесом\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = MultinomialNB()\n",
    "\n",
    "n_gram_vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "ngram_2_2_score = n_gramm_testing_score(n_gram_vectorizer, model, text, labels)\n",
    "print ngram_2_2_score\n",
    "\n",
    "n_gram_vectorizer = CountVectorizer(ngram_range=(3,3))\n",
    "ngram_3_3_score = n_gramm_testing_score(n_gram_vectorizer, model, text, labels)\n",
    "print ngram_3_3_score\n",
    "\n",
    "n_gram_vectorizer = CountVectorizer(ngram_range=(1,3))\n",
    "ngram_1_3_score = n_gramm_testing_score(n_gram_vectorizer, model, text, labels)\n",
    "print ngram_1_3_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Байес проседает по качеству на 2граммах и особенно на 3граммах сильнее, чем лог.регрессия\n",
    "\n",
    "0.65, 0.38, 0.89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85\n"
     ]
    }
   ],
   "source": [
    "# Теперь пробуем TfIdf\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "model = LogisticRegression()\n",
    "# По умолчанию на униграммах\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_score = n_gramm_testing_score(tfidf, model, text, labels)\n",
    "print tfidf_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат: 0.93 CountVectorizer vs 0.85 TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Получение более высокого качества"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Убираем стоп-слова\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words(\"english\")\n",
    "text_without_stopwords = []\n",
    "for sample in text:\n",
    "    text_without_stopwords.append(' '.join([word for word in sample.split(' ') if word not in stop_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...', u'Ok lar... Joking wif u oni...', u\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\"]\n",
      "\n",
      "[u'Go jurong point, crazy.. Available bugis n great world la e buffet... Cine got amore wat...', u'Ok lar... Joking wif u oni...', u\"Free entry 2 wkly comp win FA Cup final tkts 21st May 2005. Text FA 87121 receive entry question(std txt rate)T&C's apply 08452810075over18's\"]\n"
     ]
    }
   ],
   "source": [
    "print text[:3]\n",
    "print\n",
    "print text_without_stopwords[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Также, с помощью collections.Counter решил посмотреть на самые встречающиеся слова в текстах. \n",
    "Вдруг есть много мусора - который можно убрать.\n",
    "[(u'I', 1469),\n",
    " (u'U', 998),\n",
    " (u'', 628),\n",
    " (u'CALL', 559),\n",
    " (u'2', 457),\n",
    " (u'UR', 385),\n",
    " (u\"I'M\", 377),\n",
    " (u'GET', 375),\n",
    " (u'YOU', 295),\n",
    " (u'&LT;#&GT;', 276),\n",
    " (u'GO', 265),\n",
    " (u'4', 255),\n",
    " (u'.', 241),\n",
    " (u'LIKE', 236),\n",
    " (u'GOT', 235)]\n",
    "в топ 15 был: '&LT;#&GT;'.\n",
    "Улучшения качества не последовало.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Лемматизация\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "text_stopwords_lemmatized = []\n",
    "for sample in text_without_stopwords:\n",
    "    text_stopwords_lemmatized.append(' '.join([wnl.lemmatize(i,j[0].lower()) \n",
    "                                               if j[0].lower() in ['a','n','v'] else wnl.lemmatize(i)\n",
    "                                               for i,j in pos_tag(word_tokenize(sample))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Go jurong point , crazy.. Available bugis n great world la e buffet ... Cine get amore wat ...', u'Ok lar ... Joking wif u oni ...', u\"Free entry 2 wkly comp win FA Cup final tkts 21st May 2005 . Text FA 87121 receive entry question ( std txt rate ) T & C 's apply 08452810075over18 's\"]\n"
     ]
    }
   ],
   "source": [
    "print text_stopwords_lemmatized[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как спам связан напрямую с электронной почтой и переписками, возможно, логично использовать TweetTokenizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 48455)\n",
      "(5572, 575)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "twt = TweetTokenizer()\n",
    "vectorizer = CountVectorizer(analyzer='word', ngram_range=(1,2), tokenizer=twt.tokenize)\n",
    "X = vectorizer.fit_transform(text_stopwords_lemmatized)\n",
    "\n",
    "# Сжимаем пространство признаков\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.svm import LinearSVC\n",
    "print X.shape\n",
    "lsvc = LinearSVC(C=1, penalty=\"l1\", dual=False).fit(X, labels)\n",
    "mSelect = SelectFromModel(lsvc, prefit=True)\n",
    "X_new = mSelect.transform(X)\n",
    "print X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.954057756864\n",
      "0.982051789506\n",
      "0.987794612865\n",
      "0.99138657453\n",
      "0.991744997469\n",
      "0.99192549823\n",
      "0.992105031444\n",
      "F1 score\n",
      "0.793590595902\n",
      "0.928685860918\n",
      "0.952552331298\n",
      "0.966899141294\n",
      "0.968291260849\n",
      "0.969028533459\n",
      "0.96972744316\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Экспериментально полученный лучший результат по метрикам accuracy & f1:\n",
    "print \"Accuracy:\"\n",
    "for c in [0.01, 0.1, 1, 10, 20, 50, 100]:\n",
    "    res = cross_val_score(LogisticRegression(C=c,penalty='l2'), X_new, labels, scoring=\"accuracy\", cv=10)\n",
    "    print np.mean(res)\n",
    "print \"F1 score\"\n",
    "for c in [0.01, 0.1, 1, 10, 20, 50, 100]:\n",
    "    res = cross_val_score(LogisticRegression(C=c,penalty='l2'), X_new, labels, scoring=\"f1\", cv=10)\n",
    "    print np.mean(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 48455)\n",
      "(5, 575)\n"
     ]
    }
   ],
   "source": [
    "# А какие результаты получатся сейчас?\n",
    "test = [\n",
    "\"FreeMsg: Txt: CALL to No: 86888 & claim your reward of 3 hours talk time to use from your phone now! Subscribe6GB\",\n",
    "\"FreeMsg: Txt: claim your reward of 3 hours talk time\",\n",
    "\"Have you visited the last lecture on physics?\",\n",
    "\"Have you visited the last lecture on physics? Just buy this book and you will have all materials! Only 99$\",\n",
    "\"Only 99$\"\n",
    "]\n",
    "\n",
    "# Убираем стоп-слова\n",
    "test_sw = []\n",
    "for sample in test:\n",
    "    test_sw.append(' '.join([word for word in sample.split(' ') if word not in stop_words]))\n",
    "    \n",
    "# Лемматизация\n",
    "test_sw_lemma = []\n",
    "for sample in test_sw:\n",
    "    test_sw_lemma.append(' '.join([wnl.lemmatize(i,j[0].lower()) \n",
    "                                               if j[0].lower() in ['a','n','v'] else wnl.lemmatize(i)\n",
    "                                               for i,j in pos_tag(word_tokenize(sample))]))\n",
    "\n",
    "# Сжимаем пространство признаков\n",
    "# Обучен на исходной выборке\n",
    "test_X = vectorizer.transform(test_sw_lemma)\n",
    "print test_X.shape\n",
    "selected_test_X = mSelect.transform(test_X)\n",
    "print selected_test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(C=50)\n",
    "logreg.fit(X_new, labels)\n",
    "predicted_test = logreg.predict(selected_test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print predicted_test\n",
    "# Ничего не изменилось"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выводы:   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Качество может сильно меняться при смене корпуса. Об этом нужно помнить при переходе к tfidf\n",
    "2. Статистики по биграммам и 3граммам меньше, чем по униграммам -> классификатор может работать хуже. Но за счет регуляризации лин.классификатор не склонен сильное переобучаться. \n",
    "3. Наивный Байес страдает больше, по сравенению с лин.классификатором от нехватки статистики на 2граммах и 3граммах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
